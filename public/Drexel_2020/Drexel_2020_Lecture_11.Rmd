---
title: "Lecture 11"
subtitle: "Multiple linear regression"
author: "Jung-Jin Lee"
date: "Apr 7, 2020"
output:
  xaringan::moon_reader:
    css: ["default", "default-fonts", "Drexel.css"] 
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
 
```{r, message = F, echo = F, warning = F}
library(tidyverse) 
library(gridExtra)
library(knitr) 
library(latex2exp)
opts_chunk$set(
  tidy = FALSE,
  cache = FALSE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  dpi = 300,
  fig.align = "center",
  fig.width = 4,
  fig.height = 4
  )
filter <- dplyr::filter
options(width = 70) # controls console (and slide) output width
```

## Example: weight/girth of sheep

Download a tab-delimited file [sheep.tsv](sheep.tsv). It consists of 66 observations and 2 variables. 

```{r}
sheep <- read.table("sheep.tsv", header = T, sep = "\t") 
dim(sheep)
```

.pull-left[
```{r}
head(sheep)
```
]

.pull-right[
- `weight`: live weight of a sheep (in kg)

- `girth`: chest girth (in cm)
]

---

## Example: weight/girth of sheep

Strong linear trend is visible:

```{r, out.width = "50%"}
ggplot(sheep, aes(girth, weight)) +
  geom_point()
```

---

## Example: weight/girth of sheep

- Find the best fitting line and overlay onto the scatter plot using `geom_abline()`

--

```{r}
fit <- lm(sheep$weight ~ sheep$girth) 
# or fit <- lm(weight ~ girth, data = sheep)
```

--

```{r}
fit$coefficients
```

--

```{r}
beta0 <- fit$coefficients[1]; beta1 <- fit$coefficients[2]
beta0; beta1
```

---

## Example: weight/girth of sheep

```{r, out.width = "50%"}
ggplot(sheep, aes(girth, weight)) +
  geom_point() +
  geom_abline(intercept = beta0, slope = beta1, color = "red")
```

---

## Example: weight/girth of sheep

- Find RSS, SSreg, and TSS

--

```{r}
rss <- sum((fit$residuals)^2)
rss
```

--

```{r}
ssreg <- sum((fit$fitted.values - mean(sheep$weight))^2)
ssreg
```

--

```{r}
tss <- rss + ssreg
# or tss <- sum((sheep$weight - mean(sheep$weight))^2)
tss
```

---

## Example: weight/girth of sheep

- Compute $R^2$

--

```{r}
r2 <- ssreg/tss; print(r2)
# or alternatively
summary(fit)$r.squared
```

--

- Estimate $\sigma$

--

```{r}
sqrt(rss/(length(sheep$girth)-2))
summary(fit)$sigma # alternatively 
```

---

## Example: weight/girth of sheep

- The best fitting line equation: 
$$y = `r round(beta0, 3)` + `r round(beta1, 3)`x$$

--

- Interpretation: the live weight of a sheep with $x$ cm chest girth is approximately $`r round(beta0, 3)` + `r round(beta1, 3)`x$ kg. 

--

- For example, if there is a sheep of which chest girth is 85 cm, we can predict that its live weight would be approximately 
 $$`r round(beta0, 3)` + `r round(beta1, 3)` \times 85 = `r round(beta0, 3) + round(beta1, 3)*85` \text{ kg} $$

--

- What if there is another sheep with chest girth 86 cm? The corresponding live weight is approximately
$$`r round(beta0, 3)` + `r round(beta1, 3)` \times 86 = `r round(beta0, 3) + round(beta1, 3)*86` \text{ kg} $$
which is 1.043 more than previous. 

--

- In general $\hat{\beta}_1$ is interpreted as: a unit increase in $x$ results in the increase of $y$ by $\hat{\beta}_1$. 

---

## Making inference on $\beta_0$ and $\beta_1$

- Point estimates of $\beta_0$ and $\beta_1$ were already covered:
```{r}
fit$coefficients
```

--

- How about 95% confidence of $\beta_0$ and $\beta_1$?

```{r}
confint(fit) #<<
```

--

We are 95% certain that $\beta_1$ is between `r confint(fit)[2,1]` and `r confint(fit)[2,2]`.

---

## Making inference on $\beta_0$ and $\beta_1$

- Statistical test on $\beta_1$
  - $H_0$: $\beta_1=0$
  - $p$-value is obtained from `coefficients` of `summary` of `lm` output
  
```{r}
summary(fit)$coefficients
```

- With an extremely small $p$-value $`r summary(fit)$coefficients[2,4]`$, we conclude that $\beta_1\neq 0$.
  - Interpretation of this conclusion will be explained later

---

## Multiple linear regression

Read in data set [donkey.tsv](donkey.tsv). 

```{r}
donkey <- read.table("donkey.tsv", header = T, sep = "\t") 
dim(donkey)
head(donkey)
```

---

## Multiple linear regression

- Donkey: donkey ID

- Sex: sex

- Age: age in years

- Bodywt: body weight in kg

- Heartgirth: girth at the level of the heart (cm)

- Umbgirth: girth at the umbilicus (cm)

- Length: length from the olecranon to the tuber ischii (cm)

- Height: height at the withers (cm)

--

Researchers were interested in estimating the **weight** of donkeys using other measurements that can be obtained more easily such as girth at the level of the heart (cm), girth at the umbilicus (cm), length from the olecranon to the tuber ischii (cm), height at the withers (cm), and the age (years) and sex.

---

## Multiple linear regression

- Simple linear regression using Length

```{r, out.width = "50%"}
ggplot(donkey, aes(Length, Bodywt)) + geom_point()
```

---

## Multiple linear regression

```{r}
fit_Length <- lm(Bodywt ~ Length, data = donkey)
summary(fit_Length)
```

---

## Multiple linear regression

```{r, out.width = "50%"}
ggplot(donkey, aes(Length, Bodywt)) + geom_point() +
  geom_smooth(method = "lm", se = F)
```

---

## Multiple linear regression

- Simple linear regression using Height

```{r, out.width = "50%"}
ggplot(donkey, aes(Height, Bodywt)) + geom_point()
```

---

## Multiple linear regression

```{r}
fit_Height <- lm(Bodywt ~ Height, data = donkey)
summary(fit_Height)
```

---

## Multiple linear regression

```{r, out.width = "50%"}
ggplot(donkey, aes(Height, Bodywt)) + geom_point() +
  geom_smooth(method = "lm", se = F)
```

---

## Multiple linear regression

- In multiple linear regression, we use more than one explanatory variables. 

--

- For example, we assume that 
$$Bodywt_i=\beta_0+\beta_1 Length_i +\beta_2 Heartgirth_i +\beta_3 Height_i + e_i,$$ where $e_i$ is a random variable following $N(0,\sigma^2)$. 

--

- Again `lm()` is used for fitting

---

## Multiple linear regression

```{r}
fit <- lm(Bodywt ~ Length + Heartgirth + Height, data = donkey)
summary(fit)$coefficients
```

--

Based on the output, we conclude that 

\begin{align}
Bodywt = &-217.706 \\
  &+0.916 \times Length    \\
  &+2.124 \times Heartgirth    \\
  &+0.261 \times Height  
\end{align}

Interpretation: 0.916 represents the change in Bodywt for a unit change in Length, when **the other explanatory variables, Heartgirth and Height, are held constant** (i.e. after controlling for these variables)

---

## Model comparison

- Consider a model
$$\textbf{Model 1: } \texttt{Bodywt}=\beta_0+\beta_1 \texttt{Length}.$$
How good is this model?

--

- One approach to assess a model is to compute $R^2$.
  - A good model will yield an $R^2$ close to 1. 

```{r}
fit1 <- lm(Bodywt ~ Length, data = donkey) ## Model 1
r.squared1 <- summary(fit1)$r.squared
r.squared1
```

---

## Model comparison

- Consider another model
$$\textbf{Model 2: } \texttt{Bodywt}=\beta_0+\beta_1 \texttt{Height}.$$

```{r}
fit2 <- lm(Bodywt ~ Height, data = donkey) ## Model 2
r.squared2 <- summary(fit2)$r.squared
r.squared2
```

- **Model 1** has bigger $R^2$ than **Model 2**, so we can conclude that **Model 1** is a better model than **Model 2**.

---

## Model comparison

.pull-left[
```{r, echo = F}
ggplot(donkey, aes(Length, Bodywt)) + 
  geom_point() + 
  geom_abline(intercept = fit1$coefficients[1], slope = fit1$coefficients[2], color = "red")
```
]

.pull-right[
```{r, echo = F}
ggplot(donkey, aes(Height, Bodywt)) + 
  geom_point() + 
  geom_abline(intercept = fit2$coefficients[1], slope = fit2$coefficients[2], color = "blue")
```
]

---

## Model comparison

- Now consider the model
$$\textbf{Model 3: } \texttt{Bodywt}=\beta_0+\beta_1\texttt{Length}+\beta_2\texttt{Height}.$$
How good is this model?

--

```{r}
fit3 <- lm(Bodywt ~ Length + Height, data = donkey) ## Model 3
r.squared3 <- summary(fit3)$r.squared
r.squared3
```

- This has the biggest $R^2$ so far. Is this surprising?

--

- **Model 1** and **Model 2** are _nested_ within **Model 3**, so $R^2$ being the bigger in **Model 3** than **Model 1** or **Model 2** is expected. 

--

- In general, the more predictors are added, the bigger $R^2$ is. 

---

## Model comparison

- Now consider yet another model

\begin{align}
\textbf{Model 4: } \texttt{Bodywt} = & \beta_0 +  \beta_1\texttt{Length} + \beta_2\texttt{Height} \\
  & +\beta_3\texttt{Heartgirth} + \beta_4\texttt{Umbgirth}
\end{align}

```{r}
fit4 <- lm(Bodywt ~ Length + Height + Heartgirth + Umbgirth, data = donkey) ## Model 4
r.squared4 <- summary(fit4)$r.squared
r.squared4
```

--

- Although this model the biggest $R^2$ so far, it is more complex than other models. 

--

- Need to consider trade-off between simple model (less number of variables) vs good fit (higher $R^2$)

--

- Idea: if a bigger model does not considerably increase $R^2$, then stick to simpler model. 

